{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46086b5",
   "metadata": {},
   "source": [
    "### Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb62bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check availablity of retrieved data ...\n",
      "Retrieved data found!\n",
      "Check availablity of sampled data ...\n",
      "Sampling 2009-01\n",
      "Sampling 2009-02\n",
      "Sampling 2009-03\n",
      "Sampling 2009-04\n",
      "Sampling 2009-05\n",
      "Sampling 2009-06\n",
      "Sampling 2009-07\n",
      "Sampling 2009-08\n",
      "Sampling 2009-09\n",
      "Sampling 2009-10\n",
      "Sampling 2009-11\n",
      "Sampling 2009-12\n",
      "Sampled data saved as sampled_data/sampled_data_2009.csv\n",
      "Sampling 2010-01\n",
      "Sampling 2010-02\n",
      "Sampling 2010-03\n",
      "Sampling 2010-04\n",
      "Sampling 2010-05\n",
      "Sampling 2010-06\n",
      "Sampling 2010-07\n",
      "Sampling 2010-08\n",
      "Sampling 2010-09\n",
      "Sampling 2010-10\n",
      "Sampling 2010-11\n",
      "Sampling 2010-12\n",
      "Sampled data saved as sampled_data/sampled_data_2010.csv\n",
      "Sampling 2011-01\n",
      "Sampling 2011-02\n",
      "Sampling 2011-03\n",
      "Sampling 2011-04\n",
      "Sampling 2011-05\n",
      "Sampling 2011-06\n",
      "Sampling 2011-07\n",
      "Sampling 2011-08\n",
      "Sampling 2011-09\n",
      "Sampling 2011-10\n",
      "Sampling 2011-11\n",
      "Sampling 2011-12\n",
      "Sampled data saved as sampled_data/sampled_data_2011.csv\n",
      "Sampling 2012-01\n",
      "Sampling 2012-02\n",
      "Sampling 2012-03\n",
      "Sampling 2012-04\n",
      "Sampling 2012-05\n",
      "Sampling 2012-06\n",
      "Sampling 2012-07\n",
      "Sampling 2012-08\n",
      "Sampling 2012-09\n",
      "Sampling 2012-10\n",
      "Sampling 2012-11\n",
      "Sampling 2012-12\n",
      "Sampled data saved as sampled_data/sampled_data_2012.csv\n",
      "Sampling 2013-01\n",
      "Sampling 2013-02\n",
      "Sampling 2013-03\n",
      "Sampling 2013-04\n",
      "Sampling 2013-05\n",
      "Sampling 2013-06\n",
      "Sampling 2013-07\n",
      "Sampling 2013-08\n",
      "Sampling 2013-09\n",
      "Sampling 2013-10\n",
      "Sampling 2013-11\n",
      "Sampling 2013-12\n",
      "Sampled data saved as sampled_data/sampled_data_2013.csv\n",
      "Sampling 2014-01\n",
      "Sampling 2014-02\n",
      "Sampling 2014-03\n",
      "Sampling 2014-04\n",
      "Sampling 2014-05\n",
      "Sampling 2014-06\n",
      "Sampling 2014-07\n",
      "Sampling 2014-08\n",
      "Sampling 2014-09\n",
      "Sampling 2014-10\n",
      "Sampling 2014-11\n",
      "Sampling 2014-12\n",
      "Sampled data saved as sampled_data/sampled_data_2014.csv\n",
      "Sampling 2015-01\n",
      "Sampling 2015-02\n",
      "Sampling 2015-03\n",
      "Sampling 2015-04\n",
      "Sampling 2015-05\n",
      "Sampling 2015-06\n",
      "Sampling 2015-07\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'monthly_data/yellow_tripdata_2015-07.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mn/swf3wmj95sv9wfw9ms18gs5m0000gn/T/ipykernel_34067/2413832890.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0mfetch_taxi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myellow_taxi_links\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mtaxi_zones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_taxi_zones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mcompiled_taxi_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_and_clean_taxi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mn/swf3wmj95sv9wfw9ms18gs5m0000gn/T/ipykernel_34067/2413832890.py\u001b[0m in \u001b[0;36mcompile_and_clean_taxi_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{retrieved_files_dir}/yellow_tripdata_{year}-{month:02d}.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2011\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;31m#data = data[data['PULocationID'] >263]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesystem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         handles = get_handle(\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         )\n\u001b[1;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'monthly_data/yellow_tripdata_2015-07.parquet'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import zipfile\n",
    "import requests\n",
    "import pyarrow\n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, Time, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "\n",
    "def fetch_yellow_taxi_links(base_url):\n",
    "    resp = requests.get(base_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    return soup.find_all('a', href=re.compile(r'^(?=.*yellow_tripdata)(?=.*(\\d{4}-\\d{2}\\.parquet|\\.zip)).*$'))\n",
    "\n",
    "\n",
    "def fetch_taxi_data(links, start_date, end_date): \n",
    "    print('Check availablity of retrieved data ...')\n",
    "    if not os.path.exists(retrieved_files_dir):\n",
    "        os.makedirs(retrieved_files_dir)\n",
    "    \n",
    "    for link in links:\n",
    "        url = link['href']\n",
    "        file_name = url.split('/')[-1]\n",
    "        date_str = file_name.split('_')[-1].split('.')[0]\n",
    "        date_obj = datetime.strptime(date_str, '%Y-%m')\n",
    "        flag_load_file = True\n",
    "\n",
    "        if start_date <= date_obj <= end_date:\n",
    "            file_path = os.path.join(retrieved_files_dir, file_name)\n",
    "            \n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                print(f'Downloading {file_name}...')\n",
    "                flag_load_file = False # retrieved file unavailable\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(response.content) \n",
    "                \n",
    "            if file_name.endswith('.zip'):\n",
    "                csv_file_name = file_name.replace('.zip', '.csv')\n",
    "                csv_file_path = os.path.join(retrieved_files_dir, csv_file_name)\n",
    "\n",
    "                if not os.path.exists(csv_file_path):\n",
    "                    print(f'Extracting {file_name}...')\n",
    "                    with zipfile.ZipFile(file_path, 'r') as zip_file:\n",
    "                        zip_file.extractall(retrieved_files_diroutput_dir)\n",
    "\n",
    "                os.remove(file_path)\n",
    "    if flag_load_file:\n",
    "        print(\"Retrieved data found!\")\n",
    "    else:\n",
    "        print('Data fetching completed.')\n",
    "\n",
    "\n",
    "def clean_and_sample_data(data: pd.DataFrame, columns_to_keep: list, columns_to_rename: dict,\n",
    "                          down_threshold: float, up_threshold: float,\n",
    "                          left_threshold: float, right_threshold: float, sample_size: int, year: int) -> pd.DataFrame:\n",
    "    data = data[columns_to_keep].copy() \n",
    "    data.rename(columns={old_name: new_name for old_name, new_name in zip(columns_to_keep, columns_to_rename)}, inplace=True)\n",
    "\n",
    "    if year < 2011:\n",
    "        data = data[(data['Start_Lat'] <= up_threshold) & (data['End_Lat'] <= up_threshold) & (data['Start_Lat'] >= down_threshold) & (\n",
    "            data['End_Lat'] >= down_threshold) & (data['Start_Lon'] <= right_threshold) & (data['End_Lon'] <= right_threshold) & (\n",
    "                    data['Start_Lon'] >= left_threshold) & (data['End_Lon'] >= left_threshold)]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if data.empty:\n",
    "        return data\n",
    "    else:\n",
    "        return data.sample(sample_size, random_state=42)\n",
    " \n",
    "    return data.sample(sample_size, random_state=42)\n",
    "\n",
    "\n",
    "def compile_and_clean_taxi_data() -> pd.DataFrame:\n",
    "    yellow_taxi_data = pd.DataFrame()\n",
    "\n",
    "    down_threshold = 40.560445\n",
    "    up_threshold = 40.908524\n",
    "    left_threshold = -74.242330\n",
    "    right_threshold = -73.717047\n",
    "    sample_size = 2500\n",
    "    columns_to_rename = ['Pickup_Datetime', 'Dropoff_Datetime', \"Trip_Distance\",\n",
    "                                     \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\", \"Fare_Amt\", \"Tip_Amt\"]\n",
    "    print('Check availablity of sampled data ...')\n",
    "    if not os.path.exists(sampled_files_dir):\n",
    "        os.makedirs(sampled_files_dir)\n",
    "        \n",
    "    for year in range(2009, 2016):\n",
    "        checked = False\n",
    "        for month in range(1, 13):\n",
    "            \n",
    "            file_name = f\"sampled_data_{year}.csv\"\n",
    "            file_path = os.path.join(sampled_files_dir, file_name)\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "                # If the file exists, read in the data\n",
    "                yellow_taxi_data = pd.read_csv(file_path)\n",
    "                if not checked:\n",
    "                    print(f\"Sampled data from {file_name} loaded successfully!\") \n",
    "                checked = True\n",
    "            else:\n",
    "                # process the data as needed...\n",
    "                print(f\"Sampling {year}-{month:02d}\")   \n",
    "                if year == 2009:\n",
    "                    columns_to_keep = ['Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime', \"Trip_Distance\",\n",
    "                                       \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\", \"Fare_Amt\", \"Tip_Amt\"] \n",
    "                elif year == 2010:\n",
    "                    columns_to_keep = ['pickup_datetime', 'dropoff_datetime', \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "                                       \"dropoff_longitude\", \"dropoff_latitude\", \"fare_amount\", \"tip_amount\"] \n",
    "                if year >= 2011:\n",
    "                    columns_to_keep = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance',\n",
    "                                       'Start_Lon', 'Start_Lat', 'End_Lon', 'End_Lat', 'fare_amount', 'tip_amount']\n",
    "\n",
    "\n",
    "                data = pd.read_parquet(f\"{retrieved_files_dir}/yellow_tripdata_{year}-{month:02d}.parquet\")\n",
    "                if year >= 2011:\n",
    "                    #data = data[data['PULocationID'] >263]\n",
    "                    #data = data[data[\"DOLocationID\"] >263] \n",
    "                    data = data.merge(taxi_zones, left_on='PULocationID', right_on='LocationID', how='left') \\\n",
    "                        .rename(columns={'Lon': 'Start_Lon', 'Lat': 'Start_Lat'}) \\\n",
    "                        .drop(columns=['PULocationID', 'LocationID'])\n",
    "\n",
    "                    data = data.merge(taxi_zones, left_on='DOLocationID', right_on='LocationID', how='left') \\\n",
    "                        .rename(columns={'Lon': 'End_Lon', 'Lat': 'End_Lat'}) \\\n",
    "                        .drop(columns=['DOLocationID', 'LocationID']) \n",
    "\n",
    "                sampled_data = clean_and_sample_data(data, columns_to_keep, columns_to_rename,\n",
    "                                                     down_threshold, up_threshold, left_threshold, right_threshold,\n",
    "                                                     sample_size, year)\n",
    "\n",
    "                yellow_taxi_data = yellow_taxi_data.append(sampled_data, ignore_index=True)\n",
    "\n",
    "        yellow_taxi_data.to_csv(file_path, index=False)\n",
    "        print(f\"Sampled data saved as {file_path}\") \n",
    "        if year == 2015 and month >= 6:\n",
    "            yellow_taxi_data.to_csv(file_path, index=False)\n",
    "            break\n",
    "            \n",
    "    yellow_taxi_data.to_csv(\"sampled_taxi_data_2009_2015.csv\", index=False)\n",
    "    return yellow_taxi_data\n",
    "\n",
    "\n",
    "def load_taxi_zones():\n",
    "    zones = gpd.read_file('taxi_zone_data/taxi_zones.shp')\n",
    "    zones = zones.to_crs(epsg=2263)  \n",
    "    zones['Lon'] = zones.centroid.x\n",
    "    zones['Lat'] = zones.centroid.y\n",
    "    zones = zones.drop(columns=['OBJECTID', \"Shape_Leng\", 'Shape_Area', \"zone\", 'borough', 'geometry'])\n",
    "    return zones\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_url = 'https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page'\n",
    "    start_date = datetime(2009, 1, 1)\n",
    "    end_date = datetime(2015, 6, 30)\n",
    "    retrieved_files_dir = 'monthly_data'\n",
    "    sampled_files_dir = 'sampled_data'\n",
    "\n",
    "    yellow_taxi_links = fetch_yellow_taxi_links(main_url)\n",
    "    fetch_taxi_data(yellow_taxi_links, start_date, end_date)\n",
    "    taxi_zones = load_taxi_zones()\n",
    "    compiled_taxi_data = compile_and_clean_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9f286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f736c66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5936430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16a0281f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98facd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "def add_trip_distance_column(df):\n",
    "    def calculate_distance_from_coordinate(from_coord, to_coord):\n",
    "        R = 6371.0\n",
    "        lat1, lon1 = radians(from_coord[0]), radians(from_coord[1])\n",
    "        lat2, lon2 = radians(to_coord[0]), radians(to_coord[1])\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "        distance = R * c\n",
    "        return distance\n",
    "\n",
    "    df['trip_distance'] = df.apply(lambda r: calculate_distance_from_coordinate(r['Start_Lat'], r['Start_Lon'], r['End_Lat'], r['End_Lon']), axis=1)\n",
    "    df['trip_distance'] = df['trip_distance'].apply(lambda d: '{:.2f}'.format(d))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63618f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36ddf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860aec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1ea98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca4f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8cf6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb9127e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0d571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Define the table classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, Time, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "engine = create_engine('sqlite:///project.db')\n",
    "Base = declarative_base()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d180d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YellowTaxi(Base):\n",
    "    __tablename__ = 'yellow_taxi'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    Date = Column(Date)\n",
    "    Pickup = Column(DateTime)\n",
    "    Pickup_Time = Column(Time)\n",
    "    Trip_Distance = Column(Float)\n",
    "    Start_Lon = Column(Float)\n",
    "    Start_Lat = Column(Float)\n",
    "    End_Lon = Column(Float)\n",
    "    End_Lat = Column(Float)\n",
    "    Fare_Amt = Column(Float)\n",
    "    Tip_Amt = Column(Float)\n",
    "    Year = Column(Integer)\n",
    "    Month = Column(Integer)\n",
    "    Day = Column(Integer)\n",
    "    Time = Column(Integer)\n",
    "    DayofWeek = Column(Integer)\n",
    "\n",
    "# Define classes for Uber, HourlyWeather, and DailyWeather tables similarly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc150ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)\n",
    "\n",
    "# Load the preprocessed datasets into DataFrames:\n",
    "yellow_taxi_data = pd.read_csv(\"cleaned_yellow_taxi_data_2009_2015.csv\")\n",
    "weather_2009 = pd.read_csv(\"2009_weather.csv\")\n",
    "weather_2010 = pd.read_csv(\"2010_weather.csv\")\n",
    "weather_2011 = pd.read_csv(\"2011_weather.csv\")\n",
    "weather_2012 = pd.read_csv(\"2012_weather.csv\")\n",
    "weather_2013 = pd.read_csv(\"2013_weather.csv\")\n",
    "weather_2014 = pd.read_csv(\"2014_weather.csv\")\n",
    "weather_2015 = pd.read_csv(\"2015_weather.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
