{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ace94a",
   "metadata": {},
   "source": [
    "### Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d50956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import glob\n",
    "import zipfile\n",
    "import sqlite3 \n",
    "\n",
    "import requests\n",
    "import pyarrow\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, Date, Time, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5557b92",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670c9ee",
   "metadata": {},
   "source": [
    "### Yellow Taxi dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94332851",
   "metadata": {},
   "source": [
    "#### Downloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20c654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_yellow_taxi_links(base_url):\n",
    "    \"\"\"\n",
    "    Fetches all links to the yellow taxi data from the given base URL.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The URL to fetch the links from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of all links to the yellow taxi data.\n",
    "    \"\"\"\n",
    "    resp = requests.get(base_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    # Select all <a> elements with href attribute containing 'yellow_tripdata'\n",
    "    # and either a '.parquet' or '.zip' extension.\n",
    "    links = soup.find_all('a', href=re.compile(r'^(?=.*yellow_tripdata)(?=.*(\\d{4}-\\d{2}\\.parquet|\\.zip)).*$'))\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102fadc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_taxi_data(links, start_date, end_date, retrieved_files_dir):\n",
    "    \"\"\"\n",
    "    Downloads and extracts the yellow taxi data within the specified date range.\n",
    "\n",
    "    Args:\n",
    "        links (list): A list of links to the yellow taxi data.\n",
    "        start_date (datetime.datetime): The start date of the range to download.\n",
    "        end_date (datetime.datetime): The end date of the range to download.\n",
    "        retrieved_files_dir (str): The directory to save the downloaded files in.\n",
    "    \"\"\"\n",
    "    print('Checking availability of retrieved data...')\n",
    "    # Create the retrieved files directory if it doesn't already exist.\n",
    "    if not os.path.exists(retrieved_files_dir):\n",
    "        os.makedirs(retrieved_files_dir)\n",
    "\n",
    "    for link in links:\n",
    "        url = link['href']\n",
    "        file_name = url.split('/')[-1]\n",
    "        date_str = file_name.split('_')[-1].split('.')[0]\n",
    "        date_obj = datetime.strptime(date_str, '%Y-%m')\n",
    "\n",
    "        if start_date <= date_obj <= end_date:\n",
    "            file_path = os.path.join(retrieved_files_dir, file_name)\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"File '{file_name}' already exists.\")\n",
    "            else:\n",
    "                print(f\"Downloading '{file_name}'...\")\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "\n",
    "                if file_name.endswith('.zip'):\n",
    "                    csv_file_name = file_name.replace('.zip', '.csv')\n",
    "                    csv_file_path = os.path.join(retrieved_files_dir, csv_file_name)\n",
    "\n",
    "                    if os.path.exists(csv_file_path):\n",
    "                        print(f\"File '{csv_file_name}' already exists.\")\n",
    "                    else:\n",
    "                        print(f\"Extracting '{file_name}'...\")\n",
    "                        with zipfile.ZipFile(file_path, 'r') as zip_file:\n",
    "                            zip_file.extractall(retrieved_files_dir)\n",
    "\n",
    "                        os.remove(file_path)\n",
    "\n",
    "    print('Data fetching completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44d67c",
   "metadata": {},
   "source": [
    "#### Cleaning, filtering & sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b97bf316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_sample_data(data: pd.DataFrame, columns_to_keep: list, columns_to_rename: dict,\n",
    "                          down_threshold: float, up_threshold: float,\n",
    "                          left_threshold: float, right_threshold: float, sample_size: int, year: int) -> pd.DataFrame:\n",
    "    # Only keep the columns specified in columns_to_keep and create a copy of the DataFrame\n",
    "    data = data[columns_to_keep].copy() \n",
    "\n",
    "    # Rename the columns specified in columns_to_rename\n",
    "    data.rename(columns={old_name: new_name for old_name, new_name in zip(columns_to_keep, columns_to_rename)}, inplace=True)\n",
    "\n",
    "    if year < 2011:\n",
    "        # Only keep rows where Start_Lat, End_Lat, Start_Lon, and End_Lon are within specified thresholds\n",
    "        data = data[(data['Start_Lat'] <= up_threshold) & (data['End_Lat'] <= up_threshold) & (data['Start_Lat'] >= down_threshold) & (\n",
    "            data['End_Lat'] >= down_threshold) & (data['Start_Lon'] <= right_threshold) & (data['End_Lon'] <= right_threshold) & (\n",
    "                    data['Start_Lon'] >= left_threshold) & (data['End_Lon'] >= left_threshold)]\n",
    "    else:\n",
    "        pass  # Do nothing if year is greater than or equal to 2011\n",
    "\n",
    "    if data.empty:\n",
    "        return data\n",
    "    else:\n",
    "        # Sample the specified number of rows randomly and return the resulting DataFrame\n",
    "        return data.sample(sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb5cda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_clean_taxi_data() -> pd.DataFrame:\n",
    "    yellow_taxi_data = pd.DataFrame()\n",
    "\n",
    "    # Set threshold values for latitude and longitude coordinates\n",
    "    down_threshold = 40.560445\n",
    "    up_threshold = 40.908524\n",
    "    left_threshold = -74.242330\n",
    "    right_threshold = -73.717047\n",
    "\n",
    "    # Set the number of samples to take\n",
    "    sample_size = 2500\n",
    "\n",
    "    # Define the columns to rename\n",
    "    columns_to_rename = ['Pickup_Datetime', 'Dropoff_Datetime', \"Trip_Distance\",\n",
    "                         \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\", \"Fare_Amt\", \"Tip_Amt\"]\n",
    "\n",
    "    # Check if the directory for sampled files exists\n",
    "    if not os.path.exists(sampled_files_dir):\n",
    "        os.makedirs(sampled_files_dir)\n",
    "    print('Check availability of sampled data ...')\n",
    "\n",
    "    # Loop through the years from 2009 to 2015\n",
    "    for year in range(2009, 2016):\n",
    "        checked = False \n",
    "        # Loop through the months from 1 to 12\n",
    "        for month in range(1, 13):\n",
    "            file_name = f\"sampled_data_{year}.csv\"\n",
    "            file_path = os.path.join(sampled_files_dir, file_name)\n",
    "\n",
    "            # If the file exists, read in the data\n",
    "            if os.path.exists(file_path):\n",
    "                yellow_taxi_data = pd.read_csv(file_path)\n",
    "                if not checked:\n",
    "                    print(f\"Sampled data from {file_name} loaded successfully!\")\n",
    "                checked = True\n",
    "            else:\n",
    "                # If the file does not exist, process the data\n",
    "                print(f\"Sampling {year}-{month:02d}\")\n",
    "\n",
    "                # Define the columns to keep based on the year\n",
    "                if year == 2009:\n",
    "                    columns_to_keep = ['Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime', \"Trip_Distance\",\n",
    "                                       \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\", \"Fare_Amt\", \"Tip_Amt\"] \n",
    "                elif year == 2010:\n",
    "                    columns_to_keep = ['pickup_datetime', 'dropoff_datetime', \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "                                       \"dropoff_longitude\", \"dropoff_latitude\", \"fare_amount\", \"tip_amount\"] \n",
    "                if year >= 2011:\n",
    "                    columns_to_keep = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance',\n",
    "                                       'Start_Lon', 'Start_Lat', 'End_Lon', 'End_Lat', 'fare_amount', 'tip_amount']\n",
    "\n",
    "                # Read in the data for the given year and month\n",
    "                data = pd.read_parquet(f\"{retrieved_files_dir}/yellow_tripdata_{year}-{month:02d}.parquet\")\n",
    "\n",
    "                # If the year is 2011 or later, merge with the taxi zone data\n",
    "                if year >= 2011:\n",
    "                    data = data.merge(taxi_zones, left_on='PULocationID', right_on='LocationID', how='left') \\\n",
    "                        .rename(columns={'Lon': 'Start_Lon', 'Lat': 'Start_Lat'}) \\\n",
    "                        .drop(columns=['PULocationID', 'LocationID'])\n",
    "\n",
    "                    data = data.merge(taxi_zones, left_on='DOLocationID', right_on='LocationID', how='left') \\\n",
    "                        .rename(columns={'Lon': 'End_Lon', 'Lat': 'End_Lat'}) \\\n",
    "                        .drop(columns=['DOLocationID', 'LocationID']) \n",
    "\n",
    "                sampled_data = clean_and_sample_data(data, columns_to_keep, columns_to_rename,\n",
    "                                                     down_threshold, up_threshold, left_threshold, right_threshold,\n",
    "                                                     sample_size, year)\n",
    "\n",
    "                yellow_taxi_data = yellow_taxi_data.append(sampled_data, ignore_index=True)\n",
    "            if year == 2015 and month >= 6:\n",
    "                yellow_taxi_data.to_csv(file_path, index=False)\n",
    "                break\n",
    "        if not os.path.exists(file_path):\n",
    "            yellow_taxi_data.to_csv(file_path, index=False)\n",
    "            print(f\"Sampled data saved as {file_path}\") \n",
    "\n",
    "            \n",
    "    yellow_taxi_data.to_csv(\"sampled_taxi_data_2009_2015.csv\", index=False)\n",
    "    print(f\"Sampled Yellow Taxi dataset saved as {file_path}\") \n",
    "    return yellow_taxi_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff94b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones():\n",
    "    \"\"\"\n",
    "    Load and preprocess the NYC Taxi Zones shapefile into a GeoDataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        geopandas.GeoDataFrame: The GeoDataFrame with processed Taxi Zone data.\n",
    "    \"\"\"\n",
    "    # load the Taxi Zones shapefile into a GeoDataFrame\n",
    "    zones = gpd.read_file('taxi_zone_data/taxi_zones.shp')\n",
    "    \n",
    "    # convert the projection of the GeoDataFrame to EPSG 2263 (US feet) to match the projection of the taxi data\n",
    "    zones = zones.to_crs(epsg=2263)\n",
    "    \n",
    "    # add columns for the longitude and latitude of the centroid of each zone\n",
    "    zones['Lon'] = zones.centroid.x\n",
    "    zones['Lat'] = zones.centroid.y\n",
    "    \n",
    "    # drop columns that won't be used in the analysis\n",
    "    zones = zones.drop(columns=['OBJECTID', 'Shape_Leng', 'Shape_Area', 'zone', 'borough', 'geometry'])\n",
    "    \n",
    "    # return the processed GeoDataFrame\n",
    "    return zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98eb4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main URL to fetch the yellow taxi data links from.\n",
    "main_url = 'https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f59215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end dates for the data range to download.\n",
    "start_date = datetime(2009, 1, 1)\n",
    "end_date = datetime(2015, 6, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebca4ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the downloaded files in.\n",
    "retrieved_files_dir = 'monthly_data'\n",
    "# Define the directory to save the sampled files in.\n",
    "sampled_files_dir = 'sampled_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcc34a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking availability of retrieved data...\n",
      "File 'yellow_tripdata_2015-01.parquet' already exists.\n",
      "File 'yellow_tripdata_2015-02.parquet' already exists.\n",
      "File 'yellow_tripdata_2015-03.parquet' already exists.\n",
      "File 'yellow_tripdata_2015-04.parquet' already exists.\n",
      "File 'yellow_tripdata_2015-05.parquet' already exists.\n",
      "File 'yellow_tripdata_2015-06.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-01.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-02.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-03.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-04.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-05.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-06.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-07.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-08.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-09.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-10.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-11.parquet' already exists.\n",
      "File 'yellow_tripdata_2014-12.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-01.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-02.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-03.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-04.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-05.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-06.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-07.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-08.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-09.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-10.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-11.parquet' already exists.\n",
      "File 'yellow_tripdata_2013-12.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-01.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-02.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-03.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-04.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-05.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-06.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-07.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-08.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-09.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-10.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-11.parquet' already exists.\n",
      "File 'yellow_tripdata_2012-12.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-01.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-02.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-03.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-04.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-05.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-06.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-07.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-08.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-09.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-10.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-11.parquet' already exists.\n",
      "File 'yellow_tripdata_2011-12.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-01.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-02.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-03.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-04.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-05.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-06.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-07.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-08.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-09.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-10.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-11.parquet' already exists.\n",
      "File 'yellow_tripdata_2010-12.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-01.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-02.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-03.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-04.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-05.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-06.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-07.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-08.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-09.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-10.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-11.parquet' already exists.\n",
      "File 'yellow_tripdata_2009-12.parquet' already exists.\n",
      "Data fetching completed.\n"
     ]
    }
   ],
   "source": [
    "# Download and extract the yellow taxi data within the specified date range.\n",
    "yellow_taxi_links = fetch_yellow_taxi_links(main_url)\n",
    "fetch_taxi_data(yellow_taxi_links, start_date, end_date, retrieved_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a921deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the taxi zone lookup data.\n",
    "taxi_zones = load_taxi_zones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffefdc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check availability of sampled data ...\n",
      "Sampled data from sampled_data_2009.csv loaded successfully!\n",
      "Sampled data from sampled_data_2010.csv loaded successfully!\n",
      "Sampled data from sampled_data_2011.csv loaded successfully!\n",
      "Sampled data from sampled_data_2012.csv loaded successfully!\n",
      "Sampled data from sampled_data_2013.csv loaded successfully!\n",
      "Sampled data from sampled_data_2014.csv loaded successfully!\n",
      "Sampling 2015-01\n",
      "Sampling 2015-02\n",
      "Sampling 2015-03\n",
      "Sampling 2015-04\n",
      "Sampling 2015-05\n",
      "Sampling 2015-06\n",
      "Sampled Yellow Taxi dataset saved as sampled_data/sampled_data_2015.csv\n"
     ]
    }
   ],
   "source": [
    "# Compile and clean the downloaded taxi data, and save the sampled data to disk.\n",
    "compiled_taxi_data = compile_and_clean_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de513cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pickup_Datetime</th>\n",
       "      <th>Dropoff_Datetime</th>\n",
       "      <th>Trip_Distance</th>\n",
       "      <th>Start_Lon</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>End_Lon</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>Fare_Amt</th>\n",
       "      <th>Tip_Amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-04 07:53:32</td>\n",
       "      <td>2009-01-04 08:10:30</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-73.862767</td>\n",
       "      <td>40.769043</td>\n",
       "      <td>-73.992901</td>\n",
       "      <td>40.697823</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-25 03:29:10</td>\n",
       "      <td>2009-01-25 03:35:52</td>\n",
       "      <td>2.4</td>\n",
       "      <td>-73.977883</td>\n",
       "      <td>40.745888</td>\n",
       "      <td>-73.956754</td>\n",
       "      <td>40.772334</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-31 20:34:17</td>\n",
       "      <td>2009-01-31 20:47:42</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-73.987889</td>\n",
       "      <td>40.749865</td>\n",
       "      <td>-73.987705</td>\n",
       "      <td>40.755688</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-21 15:05:16</td>\n",
       "      <td>2009-01-21 15:21:11</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-73.990142</td>\n",
       "      <td>40.731772</td>\n",
       "      <td>-74.008403</td>\n",
       "      <td>40.725475</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-26 19:59:38</td>\n",
       "      <td>2009-01-26 20:14:16</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-73.970618</td>\n",
       "      <td>40.755777</td>\n",
       "      <td>-74.004455</td>\n",
       "      <td>40.742319</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pickup_Datetime     Dropoff_Datetime  Trip_Distance  Start_Lon  \\\n",
       "0  2009-01-04 07:53:32  2009-01-04 08:10:30           11.0 -73.862767   \n",
       "1  2009-01-25 03:29:10  2009-01-25 03:35:52            2.4 -73.977883   \n",
       "2  2009-01-31 20:34:17  2009-01-31 20:47:42            0.5 -73.987889   \n",
       "3  2009-01-21 15:05:16  2009-01-21 15:21:11            1.9 -73.990142   \n",
       "4  2009-01-26 19:59:38  2009-01-26 20:14:16            3.1 -73.970618   \n",
       "\n",
       "   Start_Lat    End_Lon    End_Lat  Fare_Amt  Tip_Amt  \n",
       "0  40.769043 -73.992901  40.697823      25.3     0.00  \n",
       "1  40.745888 -73.956754  40.772334       8.2     1.23  \n",
       "2  40.749865 -73.987705  40.755688       8.2     0.00  \n",
       "3  40.731772 -74.008403  40.725475       9.7     0.00  \n",
       "4  40.755777 -74.004455  40.742319      11.9     0.00  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac67815",
   "metadata": {},
   "source": [
    "#### distance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f71b5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    # Haversine formula for calculating the distance between two points on Earth\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad5e9f",
   "metadata": {},
   "source": [
    "#### Adding distance feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c8decb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add columns to a Pandas dataframe with distance-related features calculated using the haversine distance formula.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataframe to which distance-related features will be added.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with new columns for distance-related features.\n",
    "    \"\"\"\n",
    "    # apply the haversine_distance function to each row of the dataframe to calculate the distance between the start and end coordinates\n",
    "    data['Real_Distance'] = data.apply(lambda row: haversine_distance(row[\"Start_Lat\"], row[\"Start_Lon\"], row[\"End_Lat\"], row[\"End_Lon\"]), axis=1)\n",
    "    \n",
    "    # return the dataframe with the new columns added\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adc6f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CSV file into a Pandas dataframe\n",
    "yellow_taxi_dataset = pd.read_csv(\"sampled_taxi_data_2009_2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39a8c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the 'add_distance_features' function to the dataframe to add new columns with distance-related features\n",
    "yellow_taxi_dataset = add_distance_features(yellow_taxi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24ce2f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pickup_Datetime</th>\n",
       "      <th>Dropoff_Datetime</th>\n",
       "      <th>Trip_Distance</th>\n",
       "      <th>Start_Lon</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>End_Lon</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>Fare_Amt</th>\n",
       "      <th>Tip_Amt</th>\n",
       "      <th>Real_Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-04 07:53:32</td>\n",
       "      <td>2009-01-04 08:10:30</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-73.862767</td>\n",
       "      <td>40.769043</td>\n",
       "      <td>-73.992901</td>\n",
       "      <td>40.697823</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.525673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-25 03:29:10</td>\n",
       "      <td>2009-01-25 03:35:52</td>\n",
       "      <td>2.4</td>\n",
       "      <td>-73.977883</td>\n",
       "      <td>40.745888</td>\n",
       "      <td>-73.956754</td>\n",
       "      <td>40.772334</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1.23</td>\n",
       "      <td>3.437221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-31 20:34:17</td>\n",
       "      <td>2009-01-31 20:47:42</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-73.987889</td>\n",
       "      <td>40.749865</td>\n",
       "      <td>-73.987705</td>\n",
       "      <td>40.755688</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.647674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-21 15:05:16</td>\n",
       "      <td>2009-01-21 15:21:11</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-73.990142</td>\n",
       "      <td>40.731772</td>\n",
       "      <td>-74.008403</td>\n",
       "      <td>40.725475</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.690572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-26 19:59:38</td>\n",
       "      <td>2009-01-26 20:14:16</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-73.970618</td>\n",
       "      <td>40.755777</td>\n",
       "      <td>-74.004455</td>\n",
       "      <td>40.742319</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.219327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pickup_Datetime     Dropoff_Datetime  Trip_Distance  Start_Lon  \\\n",
       "0  2009-01-04 07:53:32  2009-01-04 08:10:30           11.0 -73.862767   \n",
       "1  2009-01-25 03:29:10  2009-01-25 03:35:52            2.4 -73.977883   \n",
       "2  2009-01-31 20:34:17  2009-01-31 20:47:42            0.5 -73.987889   \n",
       "3  2009-01-21 15:05:16  2009-01-21 15:21:11            1.9 -73.990142   \n",
       "4  2009-01-26 19:59:38  2009-01-26 20:14:16            3.1 -73.970618   \n",
       "\n",
       "   Start_Lat    End_Lon    End_Lat  Fare_Amt  Tip_Amt  Real_Distance  \n",
       "0  40.769043 -73.992901  40.697823      25.3     0.00      13.525673  \n",
       "1  40.745888 -73.956754  40.772334       8.2     1.23       3.437221  \n",
       "2  40.749865 -73.987705  40.755688       8.2     0.00       0.647674  \n",
       "3  40.731772 -74.008403  40.725475       9.7     0.00       1.690572  \n",
       "4  40.755777 -74.004455  40.742319      11.9     0.00       3.219327  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_taxi_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f8bca",
   "metadata": {},
   "source": [
    "### Uber dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50fab787",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = pd.read_csv(\"uber_rides_sample.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6718ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file) -> pd.DataFrame:\n",
    "    csv_file = csv_file.dropna()\n",
    "    \n",
    "    # Set the latitude and longitude\n",
    "    csv_file = csv_file[(csv_file['pickup_latitude'] >= 40.560445) &\n",
    "                        (csv_file['pickup_latitude'] <= 40.908524) &\n",
    "                        (csv_file['pickup_longitude'] >= -74.242330) &\n",
    "                        (csv_file['pickup_longitude'] <= -73.717047) &\n",
    "                        (csv_file['dropoff_latitude'] >= 40.560445) &\n",
    "                        (csv_file['dropoff_latitude'] <= 40.908524) &\n",
    "                        (csv_file['dropoff_longitude'] >= -74.242330) &\n",
    "                        (csv_file['dropoff_longitude'] <= -73.717047)]\n",
    "    \n",
    "    # Convert pickup_datetime column to datetime format\n",
    "    csv_file.loc[:, 'pickup_datetime'] = pd.to_datetime(csv_file['pickup_datetime'], format='%Y-%m-%d %H:%M:%S UTC')\n",
    "\n",
    "    # Create new columns for pickup date and time, as well as year, month, day, and hour\n",
    "    csv_file.loc[:, 'Pickup_Date'] = csv_file['pickup_datetime'].dt.date\n",
    "    csv_file.loc[:, 'Pickup_Time'] = csv_file['pickup_datetime'].dt.time\n",
    "    csv_file.loc[:, 'Year'] = csv_file['pickup_datetime'].dt.year\n",
    "    csv_file.loc[:, 'Month'] = csv_file['pickup_datetime'].dt.month\n",
    "    csv_file.loc[:, 'Day'] = csv_file['pickup_datetime'].dt.day\n",
    "    csv_file.loc[:, 'Hour'] = csv_file['pickup_datetime'].dt.hour\n",
    "    \n",
    "    # Convert pickup datetime to datetime object\n",
    "    csv_file.loc[:, 'Pickup'] = pd.to_datetime(csv_file['pickup_datetime'])\n",
    "    \n",
    "    # Extract day of the week from pickup datetime\n",
    "    csv_file.loc[:, 'DayofWeek'] = csv_file['Pickup'].dt.dayofweek\n",
    "    \n",
    "    # Combine latitude and longitude columns into tuples\n",
    "    csv_file['Start_point'] = list(zip(csv_file['pickup_latitude'], csv_file['pickup_longitude']))\n",
    "    csv_file['End_point'] = list(zip(csv_file['dropoff_latitude'], csv_file['dropoff_longitude']))\n",
    "    \n",
    "    csv_file = csv_file.drop(columns=['key', 'Unnamed: 0', 'passenger_count', 'Pickup_Time', 'pickup_datetime'])\\\n",
    "                 .rename(columns={'pickup_longitude': 'Start_Lon',\n",
    "                                  'pickup_latitude': 'Start_Lat',\n",
    "                                  'dropoff_longitude': 'End_Lon',\n",
    "                                  'dropoff_latitude': 'End_Lat',\n",
    "                                  'fare_amount': 'Fare_Amt',\n",
    "                                  'Pickup_Date': 'Date'})\n",
    "    \n",
    "    \n",
    "    return csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a47c043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data() -> pd.DataFrame: \n",
    "    # Load and clean the Uber dataset, and add distance features to the dataset\n",
    "    uber_dataframe = load_and_clean_uber_data(uber_data)\n",
    "    add_distance_features(uber_dataframe)\n",
    "\n",
    "    # Remove unnecessary columns from the dataset, and convert the 'Pickup' column to datetime format\n",
    "    uber_dataframe.drop(columns=[\"Start_point\", \"End_point\"], inplace=True)\n",
    "    uber_dataframe[\"Pickup\"] = pd.to_datetime(uber_dataframe[\"Pickup\"])\n",
    "\n",
    "    # Extract date from the 'Pickup' column and create a new column 'Pickup_Date'\n",
    "    uber_dataframe['Pickup_Date'] = uber_dataframe['Pickup'].dt.date\n",
    "\n",
    "    # Convert the 'Hour' and 'DayofWeek' columns to integers, and rename the 'Real_Distance' column to 'Trip_distance'\n",
    "    uber_dataframe['Hour'] = uber_dataframe['Hour'].astype(int)\n",
    "    uber_dataframe['DayofWeek'] = uber_dataframe['DayofWeek'].astype(int)\n",
    "    uber_dataframe.rename(columns={'Real_Distance': 'Trip_distance'}, inplace=True)\n",
    "\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fa87bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_dataset = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b081d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fare_Amt</th>\n",
       "      <th>Start_Lon</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>End_Lon</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Pickup</th>\n",
       "      <th>DayofWeek</th>\n",
       "      <th>Trip_distance</th>\n",
       "      <th>Pickup_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.5</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>2015-05-07</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>2015-05-07 19:52:06</td>\n",
       "      <td>3</td>\n",
       "      <td>1.683323</td>\n",
       "      <td>2015-05-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.7</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>2009-07-17</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>2009-07-17 20:04:56</td>\n",
       "      <td>4</td>\n",
       "      <td>2.457590</td>\n",
       "      <td>2009-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.9</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>2009-08-24</td>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>2009-08-24 21:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>5.036377</td>\n",
       "      <td>2009-08-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.3</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>2009-06-26</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>2009-06-26 08:22:21</td>\n",
       "      <td>4</td>\n",
       "      <td>1.661683</td>\n",
       "      <td>2009-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>2014-08-28</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>2014-08-28 17:47:00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.475450</td>\n",
       "      <td>2014-08-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fare_Amt  Start_Lon  Start_Lat    End_Lon    End_Lat        Date  Year  \\\n",
       "0       7.5 -73.999817  40.738354 -73.999512  40.723217  2015-05-07  2015   \n",
       "1       7.7 -73.994355  40.728225 -73.994710  40.750325  2009-07-17  2009   \n",
       "2      12.9 -74.005043  40.740770 -73.962565  40.772647  2009-08-24  2009   \n",
       "3       5.3 -73.976124  40.790844 -73.965316  40.803349  2009-06-26  2009   \n",
       "4      16.0 -73.925023  40.744085 -73.973082  40.761247  2014-08-28  2014   \n",
       "\n",
       "   Month  Day  Hour              Pickup  DayofWeek  Trip_distance Pickup_Date  \n",
       "0      5    7    19 2015-05-07 19:52:06          3       1.683323  2015-05-07  \n",
       "1      7   17    20 2009-07-17 20:04:56          4       2.457590  2009-07-17  \n",
       "2      8   24    21 2009-08-24 21:45:00          0       5.036377  2009-08-24  \n",
       "3      6   26     8 2009-06-26 08:22:21          4       1.661683  2009-06-26  \n",
       "4      8   28    17 2014-08-28 17:47:00          3       4.475450  2014-08-28  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b4d5a",
   "metadata": {},
   "source": [
    "### Whether Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2809f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of column names for the weather data\n",
    "WEATHER_COLUMNS = [\n",
    "    'DATE',\n",
    "    'LATITUDE',\n",
    "    'LONGITUDE',\n",
    "    \n",
    "    'Sunrise',\n",
    "    'Sunset',\n",
    "    \n",
    "    'DailyPeakWindSpeed',\n",
    "    'DailyPrecipitation',\n",
    "    'DailySustainedWindSpeed',\n",
    "    'DailyAverageWindSpeed',\n",
    "    \n",
    "    'HourlyWindSpeed',\n",
    "    'HourlyPrecipitation',   \n",
    "] \n",
    "\n",
    "\n",
    "# Define a function to get all CSV files in a directory\n",
    "def get_csv_files(directory: str) -> list:\n",
    "    return [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "\n",
    "# Define a function to merge multiple CSV files into a single DataFrame\n",
    "def merge_csv_files(files: list, columns: list) -> pd.DataFrame:\n",
    "    return pd.concat([pd.read_csv(file, usecols=columns) for file in files])\n",
    "\n",
    "\n",
    "# Define a function to split the 'DATE' column into separate 'Date' and 'Time' columns\n",
    "def split_datetime_column(weather_data: pd.DataFrame) -> None:\n",
    "    datetime_data = pd.to_datetime(weather_data['DATE'])\n",
    "    weather_data['Date'] = datetime_data.dt.date.astype(str)\n",
    "    weather_data['Time'] = datetime_data.dt.time.astype(str)\n",
    "    weather_data.drop(columns=['DATE'], inplace=True)\n",
    "\n",
    "\n",
    "# Define a function to save the merged weather data to a CSV file\n",
    "def save_weather_data(weather_data: pd.DataFrame, output_file: str) -> None:\n",
    "    weather_data.to_csv(output_file, index=False)\n",
    "    weather_data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Define a function to merge all weather files in a directory, split the 'DATE' column, and save the result to a CSV file\n",
    "def merge_weather_files() -> None:\n",
    "    files = get_csv_files('weather_data')\n",
    "    weather_data = merge_csv_files(files, WEATHER_COLUMNS)\n",
    "    split_datetime_column(weather_data)\n",
    "    save_weather_data(weather_data, 'WeatherData.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "594a9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hourly_weather_data(csv_file) -> pd.DataFrame:\n",
    "    # Select rows with Time column containing \"51:00\"\n",
    "    weather_hourly = csv_file[csv_file['Time'].str.contains(\"51:00\")]\n",
    "\n",
    "    # Extract HourlyWindSpeed, HourlyPrecipitation, Time, and Date columns into a new dataframe\n",
    "    weather_hourly = weather_hourly.loc[:, [\"HourlyWindSpeed\", \"HourlyPrecipitation\", \"Time\", \"Date\"]]\n",
    "\n",
    "    # Replace missing and \"T\" values in HourlyPrecipitation column with 0.005\n",
    "    weather_hourly[\"HourlyPrecipitation\"] = weather_hourly[\"HourlyPrecipitation\"].fillna(0).str.replace(\"T\", \"0.005\")\n",
    "\n",
    "    # Replace missing, \"T\", and \"nan\" values in HourlyWindSpeed column with 0, and convert to numeric data type\n",
    "    weather_hourly[\"HourlyWindSpeed\"] = pd.to_numeric(weather_hourly[\"HourlyWindSpeed\"].fillna(0).replace([\"T\", \"nan\"], 0.005).replace(\"nan\", 0))\n",
    "\n",
    "    # Combine Date and Time columns to create a new datetime column\n",
    "    weather_hourly['Record_Time'] = pd.to_datetime(weather_hourly['Date'] + ' ' + weather_hourly['Time'])\n",
    "\n",
    "    # Extract Year, Month, Day, and Hour from the datetime column\n",
    "    weather_hourly['Year'] = weather_hourly['Record_Time'].dt.year\n",
    "    weather_hourly['Month'] = weather_hourly['Record_Time'].dt.month\n",
    "    weather_hourly['Day'] = weather_hourly['Record_Time'].dt.day\n",
    "    weather_hourly['Hour'] = weather_hourly['Record_Time'].dt.hour\n",
    "\n",
    "    # Drop Unnecessary Columns\n",
    "    weather_hourly.drop(columns=['Time'], inplace=True)\n",
    "    weather_hourly.dropna(inplace=True)\n",
    "    return weather_hourly\n",
    "\n",
    "\n",
    "def get_daily_weather_data(csv_file) -> pd.DataFrame:\n",
    "    # Select relevant columns and replace T with 0.005\n",
    "    weather_daily = csv_file.loc[:, ['DailyPrecipitation', 'DailySustainedWindSpeed', 'DailyPeakWindSpeed', 'DailyAverageWindSpeed', 'Time', 'Date']]\n",
    "    weather_daily['DailyPrecipitation'] = weather_daily['DailyPrecipitation'].replace('T', 0.005)\n",
    "\n",
    "    # Drop rows with any missing values\n",
    "    weather_daily.dropna(inplace=True)\n",
    "\n",
    "    # Combine date and time columns to create 'Record_Time'\n",
    "    weather_daily['Record_Time'] = pd.to_datetime(weather_daily['Date'] + ' ' + weather_daily['Time'])\n",
    "    \n",
    "    # Extract year, month, day, and hour from Record_Time column\n",
    "    weather_daily['Year'], weather_daily['Month'], weather_daily['Day'], weather_daily['Hour'] = \\\n",
    "        weather_daily['Record_Time'].dt.year, weather_daily['Record_Time'].dt.month, \\\n",
    "        weather_daily['Record_Time'].dt.day, weather_daily['Record_Time'].dt.hour\n",
    "\n",
    "    # Drop the Time column\n",
    "    weather_daily.drop(columns=['Time'], inplace=True)\n",
    "    weather_daily.dropna(inplace=True)\n",
    "    return weather_daily\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_daily_sun_data(csv_file) -> pd.DataFrame:\n",
    "    \n",
    "    weather_sun = csv_file[['Sunrise', 'Sunset', 'Date']].dropna().reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    weather_sun[['Sunrise', 'Sunset']] = weather_sun[['Sunrise', 'Sunset']].apply(\n",
    "        lambda x: pd.to_datetime(x, format='%H%M').dt.time)\n",
    "\n",
    "\n",
    "\n",
    "    # Concatenate Date and Sunrise columns to create a new column Sunrise_DateTime\n",
    "    weather_sun['Sunrise_DateTime'] = pd.to_datetime(weather_sun['Date'] + ' ' + weather_sun['Sunrise'].astype(str))\n",
    "\n",
    "    # Convert Sunrise_DateTime column to Sunrise column and drop Sunrise_DateTime column\n",
    "    weather_sun['Sunrise'] = weather_sun['Sunrise_DateTime']\n",
    "    weather_sun.drop(columns=['Sunrise_DateTime'], inplace=True)\n",
    "\n",
    "    # Concatenate Date and Sunset columns to create a new column Sunset_DateTime\n",
    "    weather_sun['Sunset_DateTime'] = pd.to_datetime(weather_sun['Date'] + ' ' + weather_sun['Sunset'].astype(str))\n",
    "\n",
    "    # Convert Sunset_DateTime column to Sunset column and drop Sunset_DateTime column\n",
    "    weather_sun['Sunset'] = weather_sun['Sunset_DateTime']\n",
    "    weather_sun.drop(columns=['Sunset_DateTime'], inplace=True)\n",
    "    weather_sun.dropna(inplace=True)\n",
    "\n",
    "    return weather_sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed5e5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_weather_files()\n",
    "weather_dataset = pd.read_csv('WeatherData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "099adf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>Date</th>\n",
       "      <th>Record_Time</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>2012-01-01 18:51:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>2012-01-01 19:51:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>2012-01-01 20:51:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>2012-01-01 21:51:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2012-01-11</td>\n",
       "      <td>2012-01-11 21:51:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     HourlyWindSpeed HourlyPrecipitation        Date         Record_Time  \\\n",
       "24               3.0                0.01  2012-01-01 2012-01-01 18:51:00   \n",
       "25               7.0               0.005  2012-01-01 2012-01-01 19:51:00   \n",
       "26               9.0                0.04  2012-01-01 2012-01-01 20:51:00   \n",
       "27              11.0               0.005  2012-01-01 2012-01-01 21:51:00   \n",
       "269             11.0               0.005  2012-01-11 2012-01-11 21:51:00   \n",
       "\n",
       "     Year  Month  Day  Hour  \n",
       "24   2012      1    1    18  \n",
       "25   2012      1    1    19  \n",
       "26   2012      1    1    20  \n",
       "27   2012      1    1    21  \n",
       "269  2012      1   11    21  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the 'get_hourly_weather_data' function to get hourly weather data\n",
    "hourly_weather_dataset = get_hourly_weather_data(weather_dataset)\n",
    "hourly_weather_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "288ea96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DailyPrecipitation</th>\n",
       "      <th>DailySustainedWindSpeed</th>\n",
       "      <th>DailyPeakWindSpeed</th>\n",
       "      <th>DailyAverageWindSpeed</th>\n",
       "      <th>Date</th>\n",
       "      <th>Record_Time</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5796</th>\n",
       "      <td>0.00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2012-07-31</td>\n",
       "      <td>2012-07-31 23:59:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5850</th>\n",
       "      <td>0.64</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>2012-08-01 23:59:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>0.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2012-08-02</td>\n",
       "      <td>2012-08-02 23:59:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>0.00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2012-08-03</td>\n",
       "      <td>2012-08-03 23:59:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>0.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2012-08-04</td>\n",
       "      <td>2012-08-04 23:59:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     DailyPrecipitation  DailySustainedWindSpeed  DailyPeakWindSpeed  \\\n",
       "5796               0.00                     10.0                15.0   \n",
       "5850               0.64                      9.0                15.0   \n",
       "5876               0.00                     12.0                19.0   \n",
       "5902               0.00                      9.0                17.0   \n",
       "5935               0.00                     12.0                21.0   \n",
       "\n",
       "      DailyAverageWindSpeed        Date         Record_Time  Year  Month  Day  \\\n",
       "5796                    3.8  2012-07-31 2012-07-31 23:59:00  2012      7   31   \n",
       "5850                    2.3  2012-08-01 2012-08-01 23:59:00  2012      8    1   \n",
       "5876                    2.7  2012-08-02 2012-08-02 23:59:00  2012      8    2   \n",
       "5902                    3.5  2012-08-03 2012-08-03 23:59:00  2012      8    3   \n",
       "5935                    3.1  2012-08-04 2012-08-04 23:59:00  2012      8    4   \n",
       "\n",
       "      Hour  \n",
       "5796    23  \n",
       "5850    23  \n",
       "5876    23  \n",
       "5902    23  \n",
       "5935    23  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the 'get_daily_weather_data' function to get daily weather data\n",
    "daily_weather_dataset = get_daily_weather_data(weather_dataset)\n",
    "daily_weather_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e39adbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sunrise</th>\n",
       "      <th>Sunset</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-01 07:20:00</td>\n",
       "      <td>2012-01-01 16:39:00</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-10 07:20:00</td>\n",
       "      <td>2012-01-10 16:48:00</td>\n",
       "      <td>2012-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-11 07:20:00</td>\n",
       "      <td>2012-01-11 16:49:00</td>\n",
       "      <td>2012-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-12 07:19:00</td>\n",
       "      <td>2012-01-12 16:50:00</td>\n",
       "      <td>2012-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-13 07:19:00</td>\n",
       "      <td>2012-01-13 16:51:00</td>\n",
       "      <td>2012-01-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sunrise              Sunset        Date\n",
       "0 2012-01-01 07:20:00 2012-01-01 16:39:00  2012-01-01\n",
       "1 2012-01-10 07:20:00 2012-01-10 16:48:00  2012-01-10\n",
       "2 2012-01-11 07:20:00 2012-01-11 16:49:00  2012-01-11\n",
       "3 2012-01-12 07:19:00 2012-01-12 16:50:00  2012-01-12\n",
       "4 2012-01-13 07:19:00 2012-01-13 16:51:00  2012-01-13"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the 'get_daily_sun_data' function to get daily sunrise and sunset times\n",
    "daily_sun_dataset = get_daily_sun_data(weather_dataset)\n",
    "daily_sun_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff86ff",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52225ed",
   "metadata": {},
   "source": [
    "### create a SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bada113",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0151ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"project.db\")\n",
    "\n",
    "# Write data to the tables\n",
    "yellow_taxi_dataset.to_sql(\"yellow_taxi_database\", conn, if_exists=\"replace\", index=False)\n",
    "uber_dataset.to_sql(\"uber_database\", conn, if_exists=\"replace\", index=False)\n",
    "hourly_weather_dataset.to_sql(\"hourly_weather_database\", conn, if_exists=\"replace\", index=False)\n",
    "daily_weather_dataset.to_sql(\"daily_weather_database\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ac59ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create table\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather_database\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DailyPrecipitation FLOAT,\n",
    "    DailySustainedWindSpeed FLOAT,\n",
    "    DailyPeakWindSpeed FLOAT,\n",
    "    DailyAverageWindSpeed FLOAT,\n",
    "    Date TIMESTAMP,\n",
    "    Record_Time TIMESTAMP,\n",
    "    Year INTEGER,\n",
    "    Month INTEGER,\n",
    "    Day INTEGER,\n",
    "    Hour INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather_database\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    HourlyWindSpeed FLOAT,\n",
    "    HourlyPrecipitation FLOAT,\n",
    "    Date TIMESTAMP,\n",
    "    Record_Time TIMESTAMP,\n",
    "    Year INTEGER,\n",
    "    Month INTEGER,\n",
    "    Day INTEGER,\n",
    "    Hour INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS yellow_taxi_database\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    Date TIMESTAMP,\n",
    "    Pickup TIMESTAMP,\n",
    "    Pickup_Time TIMESTAMP,\n",
    "    Trip_Distance FLOAT,\n",
    "    Start_Lon FLOAT,\n",
    "    Start_Lat FLOAT,\n",
    "    End_Lon FLOAT,\n",
    "    End_Lat FLOAT,\n",
    "    Fare_Amt FLOAT,\n",
    "    Tip_Amt FLOAT,\n",
    "    Year INTEGER,\n",
    "    Month INTEGER,\n",
    "    Day INTEGER,\n",
    "    Time INTEGER,\n",
    "    DayofWeek INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_database\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    Fare_Amt FLOAT,\n",
    "    Start_Lon FLOAT,\n",
    "    Start_Lat FLOAT,\n",
    "    End_Lon FLOAT,\n",
    "    End_Lat FLOAT,\n",
    "    Date TIMESTAMP,\n",
    "    Year INTEGER,\n",
    "    Month INTEGER,\n",
    "    Day INTEGER,\n",
    "    Hour INTEGER,\n",
    "    Pickup TIMESTAMP,\n",
    "    DayofWeek INTEGER,\n",
    "    Trip_Distance FLOAT,\n",
    "    Pickup_Date TIMESTAMP\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2658f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the required schema.sql file\n",
    "with open(SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(\"project.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e72901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables with the schema files\n",
    "with conn:\n",
    "    conn.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    conn.execute(DAILY_WEATHER_SCHEMA)\n",
    "    conn.execute(TAXI_TRIPS_SCHEMA)\n",
    "    conn.execute(UBER_TRIPS_SCHEMA)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a836186",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a07dba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(\"project.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da62d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date Hour  Trips\n",
      "0      2013-08-22   20     17\n",
      "1      2015-04-19   14     16\n",
      "2      2009-02-14   23     15\n",
      "3      2010-02-13   20     15\n",
      "4      2013-05-04   19     15\n",
      "...           ...  ...    ...\n",
      "50796  2015-06-28   18      1\n",
      "50797  2015-06-29   01      1\n",
      "50798  2015-06-29   03      1\n",
      "50799  2015-06-30   05      1\n",
      "50800  2015-06-30   12      1\n",
      "\n",
      "[50801 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the SQL query\n",
    "query1 = \"\"\"\n",
    "    SELECT strftime('%Y-%m-%d', Pickup_Datetime) as Date, strftime('%H', Pickup_Datetime) as Hour, COUNT(*) as Trips\n",
    "    FROM yellow_taxi_database\n",
    "    WHERE strftime('%Y-%m-%d', Pickup_Datetime) BETWEEN '2009-01-01' AND '2015-06-30'\n",
    "    GROUP BY Date, Hour\n",
    "    ORDER BY Trips DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and store the results in a Pandas dataframe\n",
    "result1 = pd.read_sql_query(query1, conn)\n",
    "\n",
    "# Print the dataframe\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b4f25cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Weekday  Trips\n",
      "0     2009-12-11    127\n",
      "1     2011-04-27    125\n",
      "2     2009-10-23    123\n",
      "3     2011-06-08    123\n",
      "4     2012-03-23    122\n",
      "...          ...    ...\n",
      "2367  2014-12-25     25\n",
      "2368  2015-01-27     22\n",
      "2369  2012-10-29     21\n",
      "2370  2010-12-27     13\n",
      "2371  2011-08-28      7\n",
      "\n",
      "[2372 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the SQL query\n",
    "query2 = \"\"\"\n",
    "    SELECT Pickup_Date as Weekday, COUNT(*) as Trips\n",
    "    FROM uber_database\n",
    "    WHERE Pickup_Date BETWEEN '2009-01-01' AND '2015-06-30'\n",
    "    GROUP BY Weekday\n",
    "    ORDER BY Trips DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and store the results in a Pandas dataframe\n",
    "result2 = pd.read_sql_query(query2, conn)\n",
    "\n",
    "# Print the dataframe\n",
    "print(result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
